{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIAqr2CPdfJe"
   },
   "source": [
    "# Урок 8. Снижение размерности данных "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOcLKigsdfJg"
   },
   "source": [
    "Большая размерность данных (под ней понимается размерность пространства признаков, то есть их количество) может серьезно усложнить задачу анализа таких данных и даже стать причиной некорректной работы некоторых алгоритмов. Кроме того, часто в исходных данных могут присутствовать лишние признаки, никак не связанные с целевой переменной. Поэтому часто встает задача понижения количества признаков, оставляя при этом самые значимые (наиболее сильно влияющие на значение целевого параметра) с отсечением менее значимых (наиболее слабо коррелирующих со значением целевого параметра) или с формированием новых признаков на основе старых. То есть ставится задача перехода от пространства большей размерности к пространству меньшей размерности с сохранением максимального количества полезной информации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Pa5-7u-dfJh"
   },
   "source": [
    "## Алгоритмы снижения размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zspAp10fdfJi"
   },
   "source": [
    "Алгоритмы снижения размерности пространства признаков делятся на две группы - отбор признаков (то есть отбрасывание наименее важных признаков) и понижение размерности путем формирования новых признаков на основе старых."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lmvx3YA5dfJj"
   },
   "source": [
    "### Отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isLulqvJdfJk"
   },
   "source": [
    "Самым простым и примитивным методом отбора является _одномерный отбор признаков_. Он заключается в оценке предсказательной силы каждого признака, то есть его информативности - насколько он коррелирует с целевой переменной. Затем отбираются либо заданное количество $k$ признаков, либо те признаки, информативность которых выше некоторого порога."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U547aIwTdfJl"
   },
   "source": [
    "Оценка предсказательной силы признака (или степени связи этого признака и целевой переменной) может проводиться разными методами, например:\n",
    "\n",
    "- в случае регрессии - _корреляция_ $$R_{j} = \\frac{\\sum_{i=1}^{l}(x_{ij} - \\bar{x}_{j})(y_{i} - \\bar{y})}{\\sqrt{\\sum_{i=1}^{l}(x_{ij} - \\bar{x}_{j})^{2}\\sum_{i=1}^{l}(y_{i} - \\bar{y})^{2}}},$$ где $\\bar{x_{j}}$ и $\\bar{y}$ - среднее значение $j$-го признака и целевой переменной, соответственно. Чем больше по модулю корреляция ($\\pm 1$), тем информативнее признак. Следует заметить, что этот метод учитывает только линейную связь между признаком и целевой переменной.\n",
    "\n",
    "\n",
    "- в случае задачи классификации - _взаимная информация (mutual information)_, моделирующая корреляцию между признаками и классами. Она использует в расчете вероятность того, что одновременно значение $j$-го признака $x_{ij}$ равно числу $v$ и значение целевой переменной $y_{i}=k$, или, другими словами, долю таких объектов от общего количества объектов в выборке $P(x=v,y=k)$. Тогда взаимная информация будет находиться как $$MI_{j}=\\sum_{v \\in X}\\sum_{k \\in Y}P(x=v,y=k)\\text{log}\\frac{P(x=v,y=k)}{P(x=v)P(y=k)}.$$ Здесь $P(x=v)$ и $P(y=k)$ - доли объектов, на которых значение признака равно $v$ и значение целевой переменной равно $k$, соответственно. Если признак и целевая переменная независимы, то взаимная информация обращается в ноль. В отличие от предыдущего метода, этот метод позволяет находять произвольные зависимости (в т.ч. нелинейные) в пространстве произвольной размерности.\n",
    "\n",
    "Такие методы позволяют оценить важность исключительно каждого признака отдельно, без учета влияния комбинаций признаков на целевую переменную, поэтому они и называются одномерными. На практике зачастую признаки влияют именно в совокупности, и по отдельности могут ошибочно быть расценены как некоррелирующие с целевой переменной, поэтому одномерные методы отбора не являются оптимальным методом в большинстве случаев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DV5E5PnadfJl"
   },
   "source": [
    "Отдельной группой методов можно назвать так называемые _переборные методы_, которые дискретно оценивают качество модели, обученной на различных подмножествах признаков. При этом происходит полный перебор всех возможных вариантов. Обычно такие алгоритмы делятся на _жадные (greedy)_ и _нежадные (non-greedy)_. Полный список их можно найти в дополнительных материалах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOjQFkTgdfJm"
   },
   "source": [
    "Жадность алгоритмов заключаются в том, что если один из признаков включен в подмножество (или исключен в случае исключающего метода), в следующих итерациях поиска он уже не учитывается, так что алгоритм работает на меньшем объеме данных. Известные алгоритмы этого типа - _жадное включение_ и _жадное исключение_. В случае жадного включения на первой итерации аналогично одномерному отбору признаков находится признак, обладающий наибольшей предсказательной силой и добавляется в формирующуееся подмножество $\\{i_{1}\\}$. Далее происходит перебор оставшихся признаков с попеременным добавлением каждого из них в подмножество к первому и оценкой качества получаемой модели, обученной на подмножестве из этих двух признаков $\\{i_{1}, i_{2}\\}$. В итоге в подмножестве остается тот признак, при добавлении которого получается наилучшее качество. Далее эта процедура повторяется до момента, пока ошибка получаемой модели уменьшается. На каждой итерации в подмножество добавляется один признак, максимально улучшающий работу модели. Если на какой-то итерации при добавлении признаков ошибка не уменьшается, процесс останавливается.\n",
    "\n",
    "Плюсом такого алгоритма является относительная быстрота и возможность учета некоторых взаимодействий между признаками (как раз то, чего лишен одномерный отбор). Минусом же можно назвать вероятность застрять в локальном минимуме ошибки, если такой есть. В случае же когда есть единственный глобальный минимум, алгоритм найдет оптимальное решение.\n",
    "\n",
    "Есть также модификации этого алгоритма с многократным проходом по выборке и поочередным включением/исключением признаков из подмножества для учета совокупного влияния признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5TOTIM4dfJo"
   },
   "source": [
    "Примером нежадного алгоритма может быть простой последовательный полный перебор всех возможных подмножеств признаков. Такой подбор позволяет найти наиболее оптимальное подмножество признаков, но, очевидно, он является достаточно трудоемким (нужно перебрать $2^{n}$ вариантов, где $n$ - число признаков), поэтому подходит только для датасетов с небольшим количеством признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqDrhuvidfJp"
   },
   "source": [
    "Еще одна группа методов отбора признаков - _встроенные в модели_. Они используют эвристики, заложенные в обучающие модели, для оценки важности признаков.\n",
    "\n",
    "- Например, в случае работы с линейными моделями мы имеем зависимость целевой переменной от взвешенной суммы признаков $$a(x) = \\sum_{i=1}^{n}w_{i}x^{i}.$$ Здесь, если признаки масштабированы, веса будут являться показателями информативности признаков: чем больше вес, тем больший вклад данный признак вносит в значение целевой переменной. На основе этого показателя можно проводить отбор признаков. Также, вспоминая уроки по линейным моделям, можно упомянуть, что использование $L_{1}$-регуляризации приводит к занулению весов наименее важных признаков, то есть к их отбрасыванию, при этом больший коэффициент регуляризации будет приводить к большему количеству зануленных весов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMFGIpqQdfJr"
   },
   "source": [
    "- В случае использования решающих деревьев и их композиций, где в каждой вершине происходит разбиение на два поддерева путем сравнивания значения одного признака с некоторым значением порога, важность признака можно оценивать по тому, насколько он уменьшает значение критерия информативности, по которому оценивается качество разбиения: $$Q(X_{m}, j, t) = H(X_{m}) - \\frac{|X_{l}|}{|X_{m}|}H(X_{l}) - \\frac{|X_{r}|}{|X_{m}|}H(X_{r}),$$ где $X_{m}$ - множество объектов, попавших в вершину на данном шаге, $X_{l}$ и $X_{r}$ - множества, попадающие в левое и правое поддерево, соответственно, после разбиения. $H(X)$ - критерий информативности. \n",
    "    \n",
    "    Чем сильнее падает критерий информативности при разбиении по данному признаку (то есть чем выше $Q$), тем этот признак важнее. Таким образом, важность $j$-го признака можно оценить путем вычисления суммы уменьшений критерия информативности по всем вершинам, в которых делалось разбиение по данному признаку. Чем больше эта сумма, тем важнее данный признак был при построении дерева. В случае композиций деревьев этот показатель суммируется по всем деревьям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suyYET13dfJr"
   },
   "source": [
    "### Понижение размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8w8aSllUdfJs"
   },
   "source": [
    "Кроме отбора признаков, который не всегда оптимален в плане сохранения максимума полезной информации, существуют еще методы понижения размерности путем формирования новых признаков на основе старых. Новых признаков при использовании такого метода должно быть меньше, чем исходных, при условии сохранения максимально возможного количества информации из исходных признаков. Например, объединение нескольких признаков в линейную комбинацию:\n",
    "\n",
    "$$z_{ij}=\\sum_{k=1}^{n}w_{jk}x_{ik},$$\n",
    "\n",
    "где $x_{ij}$ - исходные признаки, $z_{ij}$ - новые принаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2iJ7XBhdfJt"
   },
   "source": [
    "Простейшим методов такого понижения размерности является метод _случайных проекций_, который заключается в преобразованиях, сохраняющих расстояния и снижающих размерности. Существование таких преобразований доказано для выборок, в которых объектов меньше, чем признаков. Веса при всех признаках в таком методе можно выбирать случайно. При этом не факт, что мы попадем в оптимальное преобразование, но практика показывает, что метод работает, если размерность нового пространства признаков\n",
    "\n",
    "$$d > \\frac{8\\text{ln}l}{\\varepsilon^{2}},$$\n",
    "\n",
    "где $l$ - количество объектов, $\\varepsilon$ - максимальное изменение расстояния между объектами (лемма о малом искажении или лемма Джонсона-Линденштрауса)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RKYQxgMdfJu"
   },
   "source": [
    "#### Метод главных компонент (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7frA6OrdfJv"
   },
   "source": [
    "Одним из наиболее известных и широко применяемых методов понижения размерности является _метод главных компонент (principal component analysis, PCA)_. Он заключается в приближении матрицы признаков матрицей меньшего ранга - так называемом низкоранговом приближении.\n",
    "\n",
    "Запишем показанную ранее формулу линейного преобразования признаков в матричном виде:\n",
    "\n",
    "$$Z = XW^{T},$$\n",
    "\n",
    "где $X$ - матрица \"объекты-признаки\", где по строкам отложены объекты, а по столбцам - значения признаков, $Z$ - матрица новых признаков, $W^{T}$ - транспонированная матрица весов. Приближение заключается формировании новой матрицы признаков $\\tilde{X}=ZW\\approx X$ с возможностью восстанавливания старых признаков по новым с максимальным уровнем точности, или, если говорить иначе, чтобы их различие было минимальным:\n",
    "\n",
    "$$\\|ZW - X\\|^{2} \\rightarrow \\underset{Z, W}{\\text{min}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oGhpkBfdfJv"
   },
   "source": [
    "При этом метод главных компонент предполагает, что матрица весов должна быть ортогональной, то есть произведение $WW^{T}$ должно равняться единичной матрице. Восстановленная матрица $ZW$ может иметь ранг меньший, чем исходная $X$, поэтому приближение будет называться низкоранговым."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P45TTDjedfJw"
   },
   "source": [
    "Геометрически метод можно представить как проецирование признаков на гиперплоскость с максимизацией дисперсии получаемой выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dV8i8cDdfJx"
   },
   "source": [
    "Если ранг матрицы исходных признаков $rank(X) \\geq d$, где $d$ - число новых признаков, то минимум функционала различия, описанного выше, достигается тогда, когда в качестве строк матрицы $W$ используются собственные векторы матрицы $X^{T}X$ , соответствующие максимальным собственным значениям $\\lambda_{1},...,\\lambda_{d}$. Максимальные собственные значения и называются главными компонентами, от чего пошло название метода. Первая главная компонента соответствует максимальному собственному значению и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvLN4XJLdfJx"
   },
   "source": [
    "Некоторые полезные свойства метода:\n",
    "\n",
    "- Матрица $Z$ при этом будет такой, что $Z^{T}Z = \\Lambda = diag(\\lambda_{1},...,\\lambda_{d})$.\n",
    "\n",
    "\n",
    "- Минимизированный функционал ошибки будет равен $$\\|ZW - X\\|^{2} = \\|X\\|^{2} - tr\\Lambda,$$ где $tr\\Lambda,$ - след матрицы $\\Lambda$, то есть сумма всех собственных значений $\\lambda_{1},...,\\lambda_{d}$, а $\\|X\\|^{2}$ - сумма всех собственных значений исходной матрицы $\\lambda_{1},...,\\lambda_{n}$, таким образом $$\\|ZW - X\\|^{2} = \\sum_{j=d+1}^{n}\\lambda_{j},$$ то есть значение функционала ошибки будет равно сумме собственных значений, которые не были взяты в получаемое разложение. Поэтому логично брать в разложение максимальные собственные значения, оставляя минимальные.\n",
    "\n",
    "\n",
    "- Матрица $X^{T}X$ - матрица ковариации, то есть матрица, которая характеризует дисперсию выборки. Дисперсия выборки после проецирования будет равна собственному значению $\\lambda$, поэтому логично, что первым берется собственный вектор, соответствующий максимальному собственному значению - нам нужно сохранить максимум дисперсии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pPzjbW8dfJy"
   },
   "source": [
    "Таким образом, для реализации метода главных компонент нужно :\n",
    "- найти собственные значения матрицы $X^{T}X$;\n",
    "- отобрать $d$ максимальных;\n",
    "- составить матрицу $W^{T}$, столбцы которой будут являться собственными векторами, соответствующими отобранным собственным значениям, расположенным в порядке убывания;\n",
    "- получить новую матрицу \"объекты-признаки\", умножив исходную матрицу $X$ на матрицу весов $W$:\n",
    "\n",
    "$$Z=XW.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnYvDiOadfJz"
   },
   "source": [
    "#### PCA и SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Thkg-tSXdfJ0"
   },
   "source": [
    "Сформулировав принцип реализации метода главных компонент, нельзя не заметить его родство с сингулярным разложением матриц (SVD). Вспомним, что сингулярное разложение матрицы - это разложение вида\n",
    "\n",
    "$$X=UDV^{T},$$\n",
    "\n",
    "где столбцы ортогональной матрицы $U$ - это собственные векторы матрицы $XX^{T}$, столбцы ортогональной матрицы $V$ - собственные векторы матрицы $X^{T}X$, а на главной диагонали диагональной матрицы $D$ расположены собственные значения матриц $XX^{T}$ и $X^{T}X$ (они равны и также называются сингулярными числами матрицы $X$).\n",
    "\n",
    "Если число новых признаков $d$ равно старому числу признаков $n$, то можно приравнять разложения\n",
    "\n",
    "$$X=ZW=UDV^{T}.$$\n",
    "\n",
    "При этом матрицы $W$ и $V^{T}$ состоят из собственных векторов матрицы $X^{T}X$, то есть они равны при $Z=UD$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tzfpn9zdfJ0"
   },
   "source": [
    "Получается, что метод главных компонент - в своем роде \"урезанная версия\" сингулярного разложения, из которого убрали минимальные собственные значения с соответствующими собственными векторами. \n",
    "Таким образом, для реализации понижения размерности методом главных компонент с помощью SVD нужно:\n",
    "- найти сингулярное разложение вектора $X$;\n",
    "- сформировать из столбцов матрицы $V$, соответствующих $d$ наибольшим сингулярным числам, матрицу весов $W$;\n",
    "- получить новую матрицу \"объекты-признаки\", умножив исходную матрицу $X$ на матрицу весов $W$:\n",
    "\n",
    "$$Z=XW.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWEQalcAdfJ1"
   },
   "source": [
    "Для закрепления теории реализуем PCA с помощью Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSmUQy4ZdfJ2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m3d93Gx2dfJ6",
    "outputId": "d1f5b908-357e-4e77-e8dd-efea75bfc7e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "# Загрузим игрушечный датасет из sklearn\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eE6X-RyIdfJ-"
   },
   "outputs": [],
   "source": [
    "# Для начала отмасштабируем выборку\n",
    "X_ = X.astype(float)\n",
    "\n",
    "rows, cols = X_.shape\n",
    "\n",
    "# центрирование - вычитание из каждого значения среднего по строке\n",
    "means = X_.mean(0)\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        X_[i, j] -= means[j]\n",
    "\n",
    "# деление каждого значения на стандартное отклонение\n",
    "std = np.std(X_, axis=0)\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        X_[j][i] /= std[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItFOmLW9dfKB",
    "outputId": "0b507805-c106-4539-d15a-4dc8327b764f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собственные значения в порядке убывания:\n",
      "437.7746724797988\n",
      "137.10457072021055\n",
      "22.013531335697195\n",
      "3.107225464292886\n"
     ]
    }
   ],
   "source": [
    "# Найдем собственные векторы и собственные значения\n",
    " \n",
    "covariance_matrix = X_.T.dot(X_)\n",
    "\n",
    "eig_values, eig_vectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "# сформируем список кортежей (собственное значение, собственный вектор)\n",
    "eig_pairs = [(np.abs(eig_values[i]), eig_vectors[:, i]) for i in range(len(eig_values))]\n",
    "\n",
    "# и отсортируем список по убыванию собственных значений\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print('Собственные значения в порядке убывания:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mdEDwm2dfKD"
   },
   "source": [
    "Оценим долю дисперсии, которая описывается найденными компонентами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ct-1I70mdfKE",
    "outputId": "ab04c5c2-649d-4f6f-e849-3791203481d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля дисперсии, описвыаемая каждой из компонент \n",
      "[72.96244541329987, 22.850761786701778, 3.6689218892828697, 0.5178709107154814]\n",
      "Кумулятивная доля дисперсии по компонентам \n",
      "[ 72.96244541  95.8132072   99.48212909 100.        ]\n"
     ]
    }
   ],
   "source": [
    "eig_sum = sum(eig_values)\n",
    "var_exp = [(i / eig_sum) * 100 for i in sorted(eig_values, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(f'Доля дисперсии, описвыаемая каждой из компонент \\n{var_exp}')\n",
    "\n",
    "# а теперя оценим кумулятивную (то есть накапливаемую) дисперсию при учитывании каждой из компонент\n",
    "print(f'Кумулятивная доля дисперсии по компонентам \\n{cum_var_exp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NE15sfLtdfKH"
   },
   "source": [
    "Таким образом, первая главная компонента описывает почти 73% информации, а первые две в сумме - 95.8%. В то же время последняя компонента описывает всего 0.5% и может быть отброжена без страха значительных потерь в качестве нашего анализа. Мы отбросим последние две компоненты, оставив первые две."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4xcGvf7dfKI",
    "outputId": "e406100f-3bd4-40fb-ce09-15db12e77a12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица весов W:\n",
      " [[ 0.52106591 -0.37741762]\n",
      " [-0.26934744 -0.92329566]\n",
      " [ 0.5804131  -0.02449161]\n",
      " [ 0.56485654 -0.06694199]]\n"
     ]
    }
   ],
   "source": [
    "# Сформируем вектор весов из собственных векторов, соответствующих первым двум главным компонентам\n",
    "W = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))\n",
    "\n",
    "print(f'Матрица весов W:\\n', W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n04tyd6xdfKL"
   },
   "outputs": [],
   "source": [
    "# Сформируем новую матрицу \"объекты-признаки\"\n",
    "Z = X_.dot(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBqTYO6udfKO",
    "outputId": "8e8e6a9d-f8fa-4c1a-ea3f-84a9bd859bed"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "y = iris.target\n",
    "for c, i in zip(\"rgb\", [0, 1, 2]):\n",
    "    plt.scatter(Z[y==i, 0], Z[y==i, 1], c=c)\n",
    "plt.xlabel('Главная компонента 1')\n",
    "plt.ylabel('Главная компонента 2')\n",
    "plt.title('PCA датасета IRIS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7P4Be8PdfKQ"
   },
   "source": [
    "Таким образом, мы перешли от четырехмерного пространства признаков к двумерному и при этом классы остались разделимы в пространстве, то есть классификация возможна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q896KWx0dfKR"
   },
   "source": [
    "PCA наиболее хорошо работает, когда собственные значения $\\lambda$ на каком-то участке графика распределения убывают скачкообразно (критерий крутого склона), другими словами, если существуют предпосылки к тому, что следует решать задачу в пространстве меньшей размерности. Если же они убывают монотонно, следует рассмотреть вариант использования других методов работы с пространством признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6YHvcPvdfKS"
   },
   "source": [
    "## Дополнительные материалы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQKbma5FdfKT"
   },
   "source": [
    "1. [Методы отбора признаков](https://habr.com/ru/company/aligntechnology/blog/303750/)\n",
    "2. [Взаимная информация](https://ru.wikipedia.org/wiki/%D0%92%D0%B7%D0%B0%D0%B8%D0%BC%D0%BD%D0%B0%D1%8F_%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D1%8F)\n",
    "3. [Методы понижения размерности](http://www.machinelearning.ru/wiki/images/0/06/SLT%2C_lecture_8.pdf)\n",
    "4. [Лемма о малом искажении](https://ru.wikipedia.org/wiki/%D0%9B%D0%B5%D0%BC%D0%BC%D0%B0_%D0%BE_%D0%BC%D0%B0%D0%BB%D0%BE%D0%BC_%D0%B8%D1%81%D0%BA%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B8)\n",
    "5. [PCA from Scratch in Python](https://github.com/bhattbhavesh91/pca-from-scratch-iris-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJqJSSgyghWZ"
   },
   "source": [
    "## Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyJOSDMJgkk2"
   },
   "source": [
    "1. Можно ли отобрать наиболее значимые признаки с помощью PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3Oi5NQ2dfKU"
   },
   "source": [
    "## Для самостоятельной работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-eNSDPndfKU"
   },
   "source": [
    "1. (*) Написать свою реализацию метода главных компонент с помощью сингулярного разложения с использованием функции [numpy.linalg.svd()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html)\n",
    "2. (*) Обучить любую модель классификации на датасете IRIS до применения PCA и после него. Сравнить качество классификации по отложенной выборке.\n",
    "3. (*) Принять участие в одном или двух соревнованиях и прислать свой псевдоним на Kaggle и ссылку на github с решением задачи.\n",
    "\n",
    "по регрессии (https://www.kaggle.com/c/tutors-expected-math-exam-results)\n",
    "\n",
    "или классификации (https://www.kaggle.com/c/choose-tutors).\n",
    "\n",
    "В скрипте можно использовать только эти импорты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvxFN6eta5kU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 1. Можно ли отобрать наиболее значимые признаки с помощью PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Необязательно, метод PCA формирует главные компоненты (новын признаки), состоящих из линейной комбинации старых признаков. Полученная в итоге матрица весов, не определяет важность признаков. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2. Обучить любую модель классификации на датасете IRIS до применения PCA и после него. Сравнить качество классификации по отложенной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация N-бутстрап выборок\n",
    "\n",
    "def get_bootstrap(data, labels, N):\n",
    "    random.seed(42)\n",
    "    n_samples = data.shape[0]\n",
    "    bootstrap = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        b_data = np.zeros(data.shape)\n",
    "        b_labels = np.zeros(labels.shape)\n",
    "        for j in range(n_samples):\n",
    "            sample_index = random.randint(0, n_samples-1)\n",
    "            b_data[j] = data[sample_index]\n",
    "            b_labels[j] = labels[sample_index]            \n",
    "        bootstrap.append((b_data, b_labels))\n",
    "        \n",
    "    return bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsample(len_sample):\n",
    "    # будем сохранять не сами признаки, а их индексы\n",
    "    sample_indexes = [i for i in range(len_sample)]\n",
    "    \n",
    "    len_subsample = int(np.sqrt(len_sample))\n",
    "    subsample = []\n",
    "    \n",
    "    random.shuffle(sample_indexes)\n",
    "    for _ in range(len_subsample):\n",
    "        subsample.append(sample_indexes.pop())\n",
    "        \n",
    "    return subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуем класс узла\n",
    "\n",
    "class Node:\n",
    "    \n",
    "    def __init__(self, index, t, true_branch, false_branch):\n",
    "        self.index = index  # индекс признака, по которому ведется сравнение с порогом в этом узле\n",
    "        self.t = t  # значение порога\n",
    "        self.true_branch = true_branch  # поддерево, удовлетворяющее условию в узле\n",
    "        self.false_branch = false_branch  # поддерево, не удовлетворяющее условию в узле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# И класс терминального узла (листа)\n",
    "\n",
    "class Leaf:\n",
    "    \n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.prediction = self.predict()\n",
    "        \n",
    "    def predict(self):\n",
    "        # подсчет количества объектов разных классов\n",
    "        classes = {}  # сформируем словарь \"класс: количество объектов\"\n",
    "        for label in self.labels:\n",
    "            if label not in classes:\n",
    "                classes[label] = 0\n",
    "            classes[label] += 1\n",
    "        #  найдем класс, количество объектов которого будет максимальным в этом листе и вернем его    \n",
    "        prediction = max(classes, key=classes.get)\n",
    "        return prediction        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчет критерия Джини\n",
    "\n",
    "def gini(labels):\n",
    "    #  подсчет количества объектов разных классов\n",
    "    classes = {}\n",
    "    for label in labels:\n",
    "        if label not in classes:\n",
    "            classes[label] = 0\n",
    "        classes[label] += 1\n",
    "    \n",
    "    #  расчет критерия\n",
    "    impurity = 1\n",
    "    for label in classes:\n",
    "        p = classes[label] / len(labels)\n",
    "        impurity -= p ** 2\n",
    "        \n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчет качества\n",
    "\n",
    "def quality(left_labels, right_labels, current_gini):\n",
    "\n",
    "    # доля выбоки, ушедшая в левое поддерево\n",
    "    p = float(left_labels.shape[0]) / (left_labels.shape[0] + right_labels.shape[0])\n",
    "    \n",
    "    return current_gini - p * gini(left_labels) - (1 - p) * gini(right_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиение датасета в узле\n",
    "\n",
    "def split(data, labels, index, t):\n",
    "    \n",
    "    left = np.where(data[:, index] <= t)\n",
    "    right = np.where(data[:, index] > t)\n",
    "        \n",
    "    true_data = data[left]\n",
    "    false_data = data[right]\n",
    "    true_labels = labels[left]\n",
    "    false_labels = labels[right]\n",
    "        \n",
    "    return true_data, false_data, true_labels, false_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нахождение наилучшего разбиения\n",
    "\n",
    "def find_best_split(data, labels):\n",
    "    \n",
    "    #  обозначим минимальное количество объектов в узле\n",
    "    min_leaf = 5\n",
    "\n",
    "    current_gini = gini(labels)\n",
    "\n",
    "    best_quality = 0\n",
    "    best_t = None\n",
    "    best_index = None\n",
    "    \n",
    "    n_features = data.shape[1]\n",
    "    \n",
    "    # выбор индекса из подвыборки длиной sqrt(n_features)\n",
    "    subsample = get_subsample(n_features)\n",
    "    \n",
    "    for index in subsample:\n",
    "        # будем проверять только уникальные значения признака, исключая повторения\n",
    "        t_values = np.unique([row[index] for row in data])\n",
    "        \n",
    "        for t in t_values:\n",
    "            true_data, false_data, true_labels, false_labels = split(data, labels, index, t)\n",
    "            #  пропускаем разбиения, в которых в узле остается менее 5 объектов\n",
    "            if len(true_data) < min_leaf or len(false_data) < min_leaf:\n",
    "                continue\n",
    "            \n",
    "            current_quality = quality(true_labels, false_labels, current_gini)\n",
    "            \n",
    "            #  выбираем порог, на котором получается максимальный прирост качества\n",
    "            if current_quality > best_quality:\n",
    "                best_quality, best_t, best_index = current_quality, t, index\n",
    "\n",
    "    return best_quality, best_t, best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение дерева с помощью рекурсивной функции\n",
    "\n",
    "def build_tree(data, labels):\n",
    "\n",
    "    quality, t, index = find_best_split(data, labels)\n",
    "\n",
    "    #  Базовый случай - прекращаем рекурсию, когда нет прироста в качества\n",
    "    if quality == 0:\n",
    "        return Leaf(data, labels)\n",
    "\n",
    "    true_data, false_data, true_labels, false_labels = split(data, labels, index, t)\n",
    "\n",
    "    # Рекурсивно строим два поддерева\n",
    "    true_branch = build_tree(true_data, true_labels)\n",
    "    false_branch = build_tree(false_data, false_labels)\n",
    "\n",
    "    # Возвращаем класс узла со всеми поддеревьями, то есть целого дерева\n",
    "    return Node(index, t, true_branch, false_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(data, labels, n_trees):\n",
    "    forest = [] # список деревьев\n",
    "    bootstrap = get_bootstrap(data, labels, n_trees) # создаем n_trees бутстреп выборок\n",
    "    \n",
    "    for b_data, b_labels in bootstrap:\n",
    "        forest.append(build_tree(b_data, b_labels)) # добавляем по дереву в ансамбль\n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция классификации отдельного объекта\n",
    "\n",
    "def classify_object(obj, node):\n",
    "\n",
    "    #  Останавливаем рекурсию, если достигли листа\n",
    "    if isinstance(node, Leaf):\n",
    "        answer = node.prediction\n",
    "        return answer\n",
    "\n",
    "    if obj[node.index] <= node.t:\n",
    "        return classify_object(obj, node.true_branch)\n",
    "    else:\n",
    "        return classify_object(obj, node.false_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция формирования предсказания по выборке на одном дереве\n",
    "\n",
    "def predict(data, tree):\n",
    "    \n",
    "    classes = []\n",
    "    for obj in data:\n",
    "        prediction = classify_object(obj, tree)\n",
    "        classes.append(prediction)\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# предсказание голосованием деревьев\n",
    "\n",
    "def tree_vote(forest, data):\n",
    "\n",
    "    # добавим предсказания всех деревьев в список\n",
    "    predictions = []\n",
    "    for tree in forest:\n",
    "        predictions.append(predict(data, tree))\n",
    "    \n",
    "    # сформируем список с предсказаниями для каждого объекта\n",
    "    predictions_per_object = list(zip(*predictions))\n",
    "    \n",
    "    # выберем в качестве итогового предсказания для каждого объекта то,\n",
    "    # за которое проголосовало большинство деревьев\n",
    "    voted_predictions = []\n",
    "    for obj in predictions_per_object:\n",
    "        voted_predictions.append(max(set(obj), key=obj.count))\n",
    "        \n",
    "    return voted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Введем функцию подсчета точности как доли правильных ответов\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьём нашу выборку на трейн и тест, и получим точность модели на исходных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.55555555555556"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_forest=random_forest(X_train, y_train, 15)\n",
    "y_pred = tree_vote(my_forest, X_test)\n",
    "accuracy_metric(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем наши исходные данные выделив 3 главные компоненты из 4 признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_svd=SVD(n_comp=3)\n",
    "my_svd.fit(X)\n",
    "X_trans=my_svd.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans, X_test_trans, y_train_trans, y_test_trans = model_selection.train_test_split(X_trans, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.11111111111111"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_forest_trans=random_forest(X_train_trans, y_train_trans, 20)\n",
    "y_pred_trans = tree_vote(my_forest_trans, X_test_trans)\n",
    "accuracy_metric(y_test_trans, y_pred_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим есть снижение точности при использовании 3 компонент, которое связано с тем, что первые три компоненты объясняют 95,5% дисперсии данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 53.52971788,  83.48653067,  95.49021308, 100.        ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_svd.cum_var_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.92306556, 11.7091661 ,  4.69185798,  1.76273239])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_svd.sing_nums()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53.529717882362405, 29.95681278402008, 12.003682417209486, 4.509786916408037]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_svd.var_exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 3. Написать свою реализацию метода главных компонент с помощью сингулярного разложения с использованием функции numpy.linalg.svd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x28f2c560b80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "cmap = ListedColormap(['red', 'green', 'blue'])\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(X[:, 1], X[:, 2], c=y, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разложение на главные компоненты с помощью сингулярного разложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD:\n",
    "    \n",
    "    #инициируем метод с указанием количества компонент, которые будут использованы в итоговом датасете(n_comp, по умолчанию-все)\n",
    "    #и указанием необходимости центрирования данных(по умолчанию - центрируем)\n",
    "    def __init__(self,n_comp: int=None, centr: bool=True):    \n",
    "        self.n_comp=n_comp\n",
    "        self.centr=centr\n",
    "    \n",
    "    #обучение\n",
    "    def fit(self, X):\n",
    "        if not self.n_comp:\n",
    "            self.n_comp=X.shape[1]\n",
    "        self.X_centr=(X-np.mean(X, axis=0))/np.std(X, axis=0) if self.centr else X.copy()\n",
    "        self.u,self.s,self.vh=np.linalg.svd(self.X_centr, full_matrices=False) #сингулярное разложение матрицы признаков\n",
    "        self.W=self.vh.T[:,:self.n_comp]  #нахождение матрицы весов        \n",
    "    \n",
    "    def transform(self):\n",
    "        x_trans=self.X_centr@self.W\n",
    "        return x_trans\n",
    "    \n",
    "    # доля объясненной дисперсии\n",
    "    def var_exp(self):\n",
    "        eig_sum = sum(self.s)\n",
    "        var_exp = [(i / eig_sum) * 100 for i in self.s]\n",
    "        return var_exp\n",
    "    \n",
    "    # накопленная объясненная дисперсия\n",
    "    def cum_var_exp(self):        \n",
    "        eig_sum = sum(self.s)\n",
    "        var_exp = [(i / eig_sum) * 100 for i in self.s]\n",
    "        return np.cumsum(var_exp)\n",
    "    \n",
    "    # сингулярные числа\n",
    "    def sing_nums(self):\n",
    "        return self.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lesson_8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
